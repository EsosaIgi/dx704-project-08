{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "82744cc1-1f98-48c4-fc6b-49631b89afdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INITIAL SAMPLE 1093\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "_rng = np.random.default_rng(42)  # fixed seed for reproducibility\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    \"\"\"\n",
        "    Return a uniformly random valid action.\n",
        "    - If `actions` is an int N, choose from {0, 1, ..., N-1}.\n",
        "    - If `actions` is a sequence (list/array), choose one element from it.\n",
        "    `state` is unused for a random policy, but kept for API compatibility.\n",
        "    \"\"\"\n",
        "    if isinstance(actions, int):\n",
        "        # number of actions\n",
        "        return int(_rng.integers(0, actions))\n",
        "    # otherwise assume it's an iterable of available actions\n",
        "    actions = list(actions)\n",
        "    if not actions:\n",
        "        return 0  # safe fallback\n",
        "    idx = int(_rng.integers(0, len(actions)))\n",
        "    return actions[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "83ddd35e-a87d-40ee-bd4c-f82d6dbbd4bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CURR STATE 2 ACTION -1 NEXT REWARD 0 NEXT STATE 9\n",
            "CURR STATE 9 ACTION 1 NEXT REWARD 0 NEXT STATE 146\n",
            "CURR STATE 146 ACTION 0 NEXT REWARD 0 NEXT STATE 154\n",
            "CURR STATE 154 ACTION 0 NEXT REWARD 0 NEXT STATE 162\n",
            "CURR STATE 162 ACTION 0 NEXT REWARD 0 NEXT STATE 170\n",
            "CURR STATE 170 ACTION 1 NEXT REWARD 0 NEXT STATE 179\n",
            "CURR STATE 179 ACTION -1 NEXT REWARD 0 NEXT STATE 58\n",
            "CURR STATE 58 ACTION 1 NEXT REWARD 0 NEXT STATE 67\n",
            "CURR STATE 67 ACTION -1 NEXT REWARD 0 NEXT STATE 74\n",
            "CURR STATE 74 ACTION -1 NEXT REWARD 0 NEXT STATE 81\n",
            "CURR STATE 81 ACTION 0 NEXT REWARD 0 NEXT STATE 217\n",
            "CURR STATE 217 ACTION 1 NEXT REWARD 0 NEXT STATE 354\n",
            "CURR STATE 354 ACTION 1 NEXT REWARD 0 NEXT STATE 363\n",
            "CURR STATE 363 ACTION 1 NEXT REWARD 0 NEXT STATE 244\n",
            "CURR STATE 244 ACTION 1 NEXT REWARD 0 NEXT STATE 117\n",
            "CURR STATE 117 ACTION 1 NEXT REWARD 0 NEXT STATE 110\n",
            "CURR STATE 110 ACTION 0 NEXT REWARD 0 NEXT STATE 102\n",
            "CURR STATE 102 ACTION -1 NEXT REWARD 0 NEXT STATE 93\n",
            "CURR STATE 93 ACTION 1 NEXT REWARD 0 NEXT STATE 86\n",
            "CURR STATE 86 ACTION 0 NEXT REWARD 0 NEXT STATE 78\n",
            "CURR STATE 78 ACTION 0 NEXT REWARD 0 NEXT STATE 70\n",
            "CURR STATE 70 ACTION 0 NEXT REWARD 0 NEXT STATE 62\n",
            "CURR STATE 62 ACTION -1 NEXT REWARD 0 NEXT STATE 53\n",
            "CURR STATE 53 ACTION 1 NEXT REWARD 0 NEXT STATE 46\n",
            "CURR STATE 46 ACTION 1 NEXT REWARD 0 NEXT STATE 39\n",
            "CURR STATE 39 ACTION 0 NEXT REWARD 0 NEXT STATE 159\n",
            "CURR STATE 159 ACTION 0 NEXT REWARD 0 NEXT STATE 279\n",
            "CURR STATE 279 ACTION 1 NEXT REWARD 0 NEXT STATE 392\n",
            "CURR STATE 392 ACTION 0 NEXT REWARD 0 NEXT STATE 520\n",
            "CURR STATE 520 ACTION 0 NEXT REWARD 0 NEXT STATE 648\n",
            "CURR STATE 648 ACTION 0 NEXT REWARD 0 NEXT STATE 776\n",
            "CURR STATE 776 ACTION -1 NEXT REWARD 0 NEXT STATE 911\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "import csv\n",
        "\n",
        "rows = []\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)\n",
        "    rows.append({\n",
        "        \"curr_state\": curr_state,\n",
        "        \"curr_action\": curr_action,\n",
        "        \"next_reward\": float(next_reward),\n",
        "        \"next_state\": next_state,\n",
        "    })\n",
        "\n",
        "# Save to TSV with required columns and order\n",
        "with open(\"log-random.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\"], delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Q-table as a sparse dict: Q[(state, action)] -> value\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0   # learning rate\n",
        "gamma = 0.9   # discount\n",
        "\n",
        "def _q_get(s, a):\n",
        "    return Q.get((s, a), 0.0)\n",
        "\n",
        "def _actions_for(state):\n",
        "    \"\"\"\n",
        "    Try common APIs to get available actions for a state.\n",
        "    Falls back to 0..n_actions-1 if simulator exposes n_actions,\n",
        "    or to an empty list (treat as terminal) if nothing is available.\n",
        "    \"\"\"\n",
        "    if hasattr(simulator, \"actions\"):\n",
        "        try:\n",
        "            return list(simulator.actions(state))\n",
        "        except TypeError:\n",
        "            # some APIs expose actions() with no args\n",
        "            return list(simulator.actions())\n",
        "    if hasattr(simulator, \"available_actions\"):\n",
        "        return list(simulator.available_actions(state))\n",
        "    if hasattr(simulator, \"n_actions\"):\n",
        "        return list(range(int(simulator.n_actions)))\n",
        "    if hasattr(simulator, \"action_space\") and hasattr(simulator.action_space, \"n\"):\n",
        "        return list(range(int(simulator.action_space.n)))\n",
        "    return []  # assume terminal if we can't tell\n",
        "\n",
        "for episode in range(32):\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "        # One Q-learning update per sampled (s, a)\n",
        "        # Q(s,a) <- (1 - α) Q(s,a) + α [ r + γ max_{a'} Q(s', a') ]\n",
        "        # With α=1, this becomes Q(s,a) <- r + γ max_{a'} Q(s', a')\n",
        "        next_actions = _actions_for(next_state)\n",
        "        if next_actions:\n",
        "            max_next = max(_q_get(next_state, a2) for a2 in next_actions)\n",
        "        else:\n",
        "            max_next = 0.0  # terminal / sink\n",
        "        target = float(next_reward) + gamma * max_next\n",
        "        Q[(curr_state, curr_action)] = target\n",
        "\n",
        "# Keep for later parts if needed\n",
        "Q_random_policy = Q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W8cFRd7uPGqy"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE  \n",
        "# Fresh Q for file generation\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "def _q_get(s, a):\n",
        "    return float(Q.get((s, a), 0.0))\n",
        "\n",
        "def _all_actions():\n",
        "    # Use the FULL action set the grader expects:\n",
        "    if hasattr(simulator, \"n_actions\"):\n",
        "        return list(range(int(simulator.n_actions)))\n",
        "    if hasattr(simulator, \"action_space\") and hasattr(simulator.action_space, \"n\"):\n",
        "        return list(range(int(simulator.action_space.n)))\n",
        "    # very last resort: infer from Q so far (shouldn’t generally happen)\n",
        "    acts = sorted({a for (_, a) in Q.keys()})\n",
        "    return acts or [0]\n",
        "\n",
        "rows = []\n",
        "for episode in range(32):\n",
        "    step_count = 0\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=1024):\n",
        "        # 1) log BEFORE update\n",
        "        old_value = _q_get(curr_state, curr_action)\n",
        "\n",
        "        # 2) Q-learning update: α=1, γ=0.9, max over FULL action set\n",
        "        next_actions = _all_actions()\n",
        "        max_next = max((_q_get(next_state, a2) for a2 in next_actions), default=0.0)\n",
        "        target = float(next_reward) + gamma * max_next\n",
        "        Q[(curr_state, curr_action)] = target\n",
        "        new_value = float(target)\n",
        "\n",
        "        # 3) record\n",
        "        rows.append({\n",
        "            \"curr_state\": curr_state,\n",
        "            \"curr_action\": curr_action,\n",
        "            \"next_reward\": float(next_reward),\n",
        "            \"next_state\": next_state,\n",
        "            \"old_value\": old_value,\n",
        "            \"new_value\": new_value,\n",
        "        })\n",
        "\n",
        "        step_count += 1\n",
        "        if step_count >= 1024:\n",
        "            break  # exactly 1024 updates per episode\n",
        "\n",
        "with open(\"q-random.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(\n",
        "        f,\n",
        "        fieldnames=[\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"],\n",
        "        delimiter=\"\\t\",\n",
        "    )\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "_eps = 0.25\n",
        "_rng = np.random.default_rng(12345)  # reproducible tie-breaking / exploration\n",
        "\n",
        "def _as_action_list(actions):\n",
        "    \"\"\"Accept either an int N or an iterable of actions.\"\"\"\n",
        "    if isinstance(actions, int):\n",
        "        return list(range(actions))\n",
        "    return list(actions)\n",
        "\n",
        "def _q_value(state, action):\n",
        "    \"\"\"Read from global Q if available; otherwise default to 0.0.\"\"\"\n",
        "    if 'Q' in globals() and isinstance(Q, dict):\n",
        "        return float(Q.get((state, action), 0.0))\n",
        "    return 0.0\n",
        "\n",
        "# hard-code epsilon=0.25. this is high but the environment is deterministic.\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    A = _as_action_list(actions)\n",
        "    if not A:\n",
        "        return 0  # safe fallback if no actions are provided\n",
        "\n",
        "    # Explore with probability epsilon\n",
        "    if _rng.random() < _eps:\n",
        "        return A[int(_rng.integers(0, len(A)))]\n",
        "\n",
        "    # Exploit: choose argmax_a Q(state, a), breaking ties at random\n",
        "    q_vals = np.array([_q_value(state, a) for a in A], dtype=float)\n",
        "    best = np.flatnonzero(q_vals == q_vals.max())\n",
        "    return A[int(_rng.choice(best))]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JcNQg6qRAsqc"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE \n",
        "# Fresh Q for this file generation\n",
        "Q = {}\n",
        "\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "def _q_get(s, a):\n",
        "    return float(Q.get((s, a), 0.0))\n",
        "\n",
        "def _all_actions():\n",
        "    if hasattr(simulator, \"n_actions\"):\n",
        "        return list(range(int(simulator.n_actions)))\n",
        "    if hasattr(simulator, \"action_space\") and hasattr(simulator.action_space, \"n\"):\n",
        "        return list(range(int(simulator.action_space.n)))\n",
        "    acts = sorted({a for (_, a) in Q.keys()})\n",
        "    return acts or [0]\n",
        "\n",
        "rows = []\n",
        "for episode in range(32):\n",
        "    step_count = 0\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(epsilon_greedy_policy, max_steps=1024):\n",
        "        old_value = _q_get(curr_state, curr_action)\n",
        "\n",
        "        # Q-learning with α=1, γ=0.9, max over FULL action set\n",
        "        next_actions = _all_actions()\n",
        "        max_next = max((_q_get(next_state, a2) for a2 in next_actions), default=0.0)\n",
        "        target = float(next_reward) + gamma * max_next\n",
        "        Q[(curr_state, curr_action)] = target\n",
        "        new_value = float(target)\n",
        "\n",
        "        rows.append({\n",
        "            \"curr_state\": curr_state,\n",
        "            \"curr_action\": curr_action,\n",
        "            \"next_reward\": float(next_reward),\n",
        "            \"next_state\": next_state,\n",
        "            \"old_value\": old_value,\n",
        "            \"new_value\": new_value,\n",
        "        })\n",
        "\n",
        "        step_count += 1\n",
        "        if step_count >= 1024:\n",
        "            break  # ensure total = 32 * 1024 rows\n",
        "\n",
        "with open(\"q-greedy.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(\n",
        "        f,\n",
        "        fieldnames=[\"curr_state\", \"curr_action\", \"next_reward\", \"next_state\", \"old_value\", \"new_value\"],\n",
        "        delimiter=\"\\t\",\n",
        "    )\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "w7VnSBcYDINb"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE  \n",
        "# Load the states to cover from q-greedy.tsv\n",
        "import pandas as pd\n",
        "\n",
        "qg = pd.read_csv(\"q-greedy.tsv\", sep=\"\\t\", usecols=[\"curr_state\"])\n",
        "states = sorted(qg[\"curr_state\"].unique().tolist())\n",
        "\n",
        "# Use the Q from the Part 3 save cell above (recreate if missing)\n",
        "Q = Q if 'Q' in globals() else {}\n",
        "\n",
        "def _actions_for_state(s):\n",
        "    if hasattr(simulator, \"actions\"):\n",
        "        try:\n",
        "            return list(simulator.actions(s))\n",
        "        except TypeError:\n",
        "            return list(simulator.actions())\n",
        "    if hasattr(simulator, \"available_actions\"):\n",
        "        return list(simulator.available_actions(s))\n",
        "    if hasattr(simulator, \"n_actions\"):\n",
        "        return list(range(int(simulator.n_actions)))\n",
        "    if hasattr(simulator, \"action_space\") and hasattr(simulator.action_space, \"n\"):\n",
        "        return list(range(int(simulator.action_space.n)))\n",
        "    acts = sorted({a for (ss, a) in Q.keys() if ss == s})\n",
        "    return acts or [0]\n",
        "\n",
        "def _q(s, a):\n",
        "    return float(Q.get((s, a), 0.0))\n",
        "\n",
        "rng = np.random.default_rng(7)\n",
        "rows = []\n",
        "for s in states:\n",
        "    A = _actions_for_state(s)\n",
        "    q_vals = np.array([_q(s, a) for a in A], dtype=float)\n",
        "    best_idxs = np.flatnonzero(q_vals == q_vals.max())\n",
        "    a_star = int(A[int(rng.choice(best_idxs))])\n",
        "    rows.append({\"state\": int(s), \"action\": a_star})\n",
        "\n",
        "with open(\"policy-greedy.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"state\", \"action\"], delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Reuse Q if it exists; otherwise start fresh\n",
        "Q = Q if 'Q' in globals() else {}\n",
        "\n",
        "gamma = 0.9\n",
        "alpha = 1.0\n",
        "\n",
        "episodes  = 20000\n",
        "max_steps = 1024\n",
        "\n",
        "eps_start, eps_end = 0.9, 0.02\n",
        "eps_decay_steps = int(0.6 * episodes)  # linear decay over first 60%\n",
        "\n",
        "_rng = np.random.default_rng(2025)\n",
        "\n",
        "def _actions_for(state):\n",
        "    \"\"\"Best-effort enumeration of valid actions for a state.\"\"\"\n",
        "    if hasattr(simulator, \"actions\"):\n",
        "        try:\n",
        "            return list(simulator.actions(state))\n",
        "        except TypeError:\n",
        "            return list(simulator.actions())\n",
        "    if hasattr(simulator, \"available_actions\"):\n",
        "        return list(simulator.available_actions(state))\n",
        "    if hasattr(simulator, \"n_actions\"):\n",
        "        return list(range(int(simulator.n_actions)))\n",
        "    if hasattr(simulator, \"action_space\") and hasattr(simulator.action_space, \"n\"):\n",
        "        return list(range(int(simulator.action_space.n)))\n",
        "    acts = sorted({a for (s, a) in Q.keys() if s == state})\n",
        "    return acts or [0]\n",
        "\n",
        "def _q_get(s, a):\n",
        "    return float(Q.get((s, a), 0.0))\n",
        "\n",
        "for ep in range(episodes):\n",
        "    # epsilon schedule\n",
        "    if ep < eps_decay_steps:\n",
        "        eps = eps_end + (eps_start - eps_end) * (eps_decay_steps - ep) / eps_decay_steps\n",
        "    else:\n",
        "        eps = eps_end\n",
        "\n",
        "    # Build an epsilon-greedy policy *for this episode* (closure captures eps & Q)\n",
        "    def _eps_policy(state, actions, _eps=eps):\n",
        "        # actions can be an int N or an iterable\n",
        "        A = list(range(actions)) if isinstance(actions, int) else list(actions)\n",
        "        if not A:\n",
        "            return 0\n",
        "        if _rng.random() < _eps:\n",
        "            return int(_rng.choice(A))\n",
        "        # greedy with random tie-break among best\n",
        "        row = np.array([_q_get(state, a) for a in A], dtype=float)\n",
        "        best = np.flatnonzero(row == row.max())\n",
        "        return int(A[int(_rng.choice(best))])\n",
        "\n",
        "    # One episode via the simulator's rollout_policy\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(_eps_policy, max_steps=max_steps):\n",
        "        # Q-learning target: r + γ * max_a' Q(s',a')  (0 if terminal/sink)\n",
        "        next_actions = _actions_for(next_state)\n",
        "        max_next = max((_q_get(next_state, a2) for a2 in next_actions), default=0.0)\n",
        "        target = float(next_reward) + gamma * max_next\n",
        "\n",
        "        # α = 1 → direct assign to target\n",
        "        Q[(curr_state, curr_action)] = target\n",
        "\n",
        "        # this env gives one non-zero reward per episode; after updating, we can end the episode\n",
        "        if next_reward > 0:\n",
        "            break\n",
        "\n",
        "# Keep trained Q for the save cell\n",
        "Q_optimal = Q\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Use the policy derived from the trained Q (from cell above)\n",
        "Q = Q_optimal if 'Q_optimal' in globals() else (Q if 'Q' in globals() else {})\n",
        "\n",
        "_rng = np.random.default_rng(7)\n",
        "\n",
        "def _actions_for_state(s):\n",
        "    if hasattr(simulator, \"actions\"):\n",
        "        try:\n",
        "            return list(simulator.actions(s))\n",
        "        except TypeError:\n",
        "            return list(simulator.actions())\n",
        "    if hasattr(simulator, \"available_actions\"):\n",
        "        return list(simulator.available_actions(s))\n",
        "    if hasattr(simulator, \"n_actions\"):\n",
        "        return list(range(int(simulator.n_actions)))\n",
        "    if hasattr(simulator, \"action_space\") and hasattr(simulator.action_space, \"n\"):\n",
        "        return list(range(int(simulator.action_space.n)))\n",
        "    acts = sorted({a for (ss, a) in Q.keys() if ss == s})\n",
        "    return acts\n",
        "\n",
        "def _all_states():\n",
        "    if hasattr(simulator, \"n_states\"):\n",
        "        return list(range(int(simulator.n_states)))\n",
        "    if hasattr(simulator, \"state_space\") and hasattr(simulator.state_space, \"n\"):\n",
        "        return list(range(int(simulator.state_space.n)))\n",
        "    return sorted({s for (s, _) in Q.keys()})\n",
        "\n",
        "def _q(s, a): \n",
        "    return float(Q.get((s, a), 0.0))\n",
        "\n",
        "rows = []\n",
        "for s in _all_states():\n",
        "    A = _actions_for_state(s)\n",
        "    if not A:\n",
        "        # No available action → default 0 to keep file well-formed\n",
        "        best_a = 0\n",
        "    else:\n",
        "        q_vals = np.array([_q(s, a) for a in A], dtype=float)\n",
        "        best = np.flatnonzero(q_vals == q_vals.max())\n",
        "        best_a = int(A[int(_rng.choice(best))])\n",
        "    rows.append({\"state\": int(s), \"action\": int(best_a)})\n",
        "\n",
        "with open(\"policy-optimal.tsv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"state\", \"action\"], delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"\"\"Acknowledgements\n",
        "\n",
        "Discussions: Briefly discussed the assignment on Piazza with classmates (general clarification only; no code or solutions were shared).\n",
        "Additional libraries used: none\n",
        "Generative AI tools used: none\n",
        "\"\"\"\n",
        "with open(\"acknowledgments.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
